# 嵌入式陪伴机器人部署 Prompt

> 使用此 prompt 在 RK3568 主控上从零开始部署语音陪伴机器人

---

## 项目背景

我正在将一个已调试完成的 Python 语音助手项目，移植到嵌入式机器人平台上。

### 目标平台
- **主控芯片**: RK3568 (aarch64)
- **操作系统**: Ubuntu 20.04.5 LTS (GNU/Linux 4.19.232 aarch64)
- **网络**: WiFi 已连接
- **显示**: 头部 LCD 屏幕 (通过 MIPI 连接)

### 硬件架构
```
┌─────────────────────────────────────────────────────────────┐
│                          头部                                │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ LCD 显示屏 (MIPI)  │  摄像头 (USB)  │  头部舵机 (PWM) │   │
│  └──────────────────────────────────────────────────────┘   │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ 降噪板/麦克风 (USB串口)  │  USB声卡 (USB)              │   │
│  └──────────────────────────────────────────────────────┘   │
│                          │                                   │
│                     RK3568 主板                              │
│                          │                                   │
├─────────────────────────────────────────────────────────────┤
│                          机身                                │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ 扬声器 (LP/LN)  │  左右手臂舵机 (PWM, 6V)             │   │
│  └──────────────────────────────────────────────────────┘   │
├─────────────────────────────────────────────────────────────┤
│                          底盘                                │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ 雷达 (UART)  │  底盘电机 (12V/24V)  │  电池 (24V)     │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

---

## 已完成功能（可复用代码）

以下功能已在 Windows PC 上调试完成，需要移植到嵌入式平台：

### 1. 核心语音交互
| 模块 | 说明 | 源文件 |
|-----|------|-------|
| 语音识别 (ASR) | 豆包流式语音识别模型2.0，WebSocket 双向流 | `voice_assistant.py` |
| 智能对话 (Chat) | Doubao-Seed-1.6 推理模型，HTTP 流式 | `voice_assistant.py` |
| 语音合成 (TTS) | 豆包语音合成模型2.0，WebSocket 流式 | `voice_assistant.py` |

### 2. 本地识别模块
| 模块 | 说明 | 源文件 |
|-----|------|-------|
| 人脸识别 | face_recognition 库，本地人脸检测与识别 | `face_recognition_utils.py` |
| 物体检测 | YOLOv8n，80类物体检测（中文标签）| `object_detection_utils.py` |
| 声纹识别 | Resemblyzer d-vector，说话人识别 | `speaker_recognition_utils.py` |

### 3. 辅助功能
| 模块 | 说明 | 源文件 |
|-----|------|-------|
| 意图识别 | 规则化意图匹配（看看、声纹识别等）| `intent_handler.py` |
| 智能记忆 | Mem0 HTTP API，长期记忆管理 | `mem0_client.py` |
| 摄像头拍照 | OpenCV 采集 | `camera_utils.py` |

### 4. 配置文件
| 文件 | 说明 |
|-----|------|
| `config.py` | 所有非敏感配置参数 |
| `api_secrets.py` | API 密钥（需创建）|
| `api_secrets.example.py` | API 密钥模板 |

---

## 移植要点

### 需要修改的部分

1. **去掉 PyQt6 GUI 依赖**
   - 原项目使用 PyQt6 做桌面界面
   - 嵌入式平台改为**无界面模式**或**简化 LCD 显示**
   - 交互改为：按键/语音唤醒 → 录音 → 处理 → 播放

2. **音频输入/输出适配**
   - 麦克风：降噪板通过 USB 串口连接
   - 扬声器：通过 USB 声卡输出
   - 需要确认 ALSA 设备配置

3. **摄像头适配**
   - USB 摄像头连接
   - OpenCV 采集需确认 /dev/video* 设备

4. **aarch64 依赖安装**
   - dlib/face_recognition 需要从源码编译
   - PyTorch/ultralytics 需要 ARM64 版本
   - resemblyzer 依赖 webrtcvad（可能需要编译）

### 保持不变的部分

1. **API 调用逻辑**：ASR/TTS/Chat 的 WebSocket/HTTP 代码可直接复用
2. **意图识别逻辑**：`intent_handler.py` 无需修改
3. **记忆服务客户端**：`mem0_client.py` 无需修改
4. **配置参数**：`config.py` 大部分可复用

---

## 功能目标

请帮我实现以下陪伴机器人功能：

### 第一阶段：基础语音交互
- [ ] 语音唤醒或按键触发录音
- [ ] 流式语音识别 (ASR)
- [ ] 调用大模型对话 (Chat)
- [ ] 流式语音合成并播放 (TTS)
- [ ] 静音检测自动结束录音

### 第二阶段：多模态 AI 大脑
- [ ] 咨询问答：通用知识问答能力
- [ ] 新闻查询：接入新闻 API 或联网搜索
- [ ] 情感陪聊：温暖、有同理心的对话风格
- [ ] 图文理解：拍照 + 大模型分析场景

### 第三阶段：声纹识别与身份系统
- [ ] 声纹识别：识别当前说话人是谁
- [ ] 声纹注册：首次对话后询问并记录
- [ ] 人脸识别：拍照识别已注册用户
- [ ] 身份关联：声纹+人脸融合识别

### 第四阶段：长期记忆系统
- [ ] 记忆存储：存储用户喜好、关系到 Mem0
- [ ] 重要信息：记录生日、纪念日等特殊日期
- [ ] 上下文注入：对话前搜索相关记忆
- [ ] 主动提醒：纪念日/生日主动提醒功能

### 第五阶段：性格成长体系
- [ ] 个性定制：可配置的性格特征（活泼/稳重/幽默）
- [ ] 性格塑造：根据用户互动逐渐调整风格
- [ ] 情感状态：模拟情绪变化（开心/疲惫/好奇）

### 第六阶段：智能视觉系统
- [ ] 物体检测：YOLO 识别周围物体
- [ ] 人脸检测：检测画面中的人脸
- [ ] 场景理解：识别当前环境（室内/室外/厨房等）
- [ ] 意图触发：识别"看看"、"这是谁"等指令

### 第七阶段：声源定位
- [ ] 麦克风阵列：多麦克风声源定位
- [ ] 方向识别：判断声音来自哪个方向
- [ ] 头部追踪：控制头部舵机朝向声源

### 第八阶段：自主行为系统
- [ ] LCD 显示状态/表情动画
- [ ] 头部舵机：点头、摇头、转向
- [ ] 手臂舵机：挥手、比心等动作
- [ ] 底盘运动：自主移动、跟随用户
- [ ] 雷达避障：SLAM 导航与避障

---

## API 配置信息

使用火山引擎（豆包）服务：

```python
# === 语音识别 (ASR) ===
ASR_WS_URL = "wss://openspeech.bytedance.com/api/v3/sauc/bigmodel_async"
ASR_RESOURCE_ID = "volc.seedasr.sauc.duration"  # 2.0版本

# === 对话模型 (Chat) ===
CHAT_API_URL = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
CHAT_MODEL_NAME = "doubao-seed-1-6-251015"

# === 语音合成 (TTS) ===
TTS_WS_URL = "wss://openspeech.bytedance.com/api/v3/tts/bidirection"
TTS_RESOURCE_ID = "seed-tts-2.0"
TTS_SPEAKER = "zh_female_xiaohe_uranus_bigtts"  # 小何2.0音色

# === 记忆服务 (Mem0) ===
MEM0_BASE_URL = "http://tenyuan.tech:9000"
```

---

## 请求帮助

请帮我完成以下任务：

1. **项目结构设计**
   - 设计适合嵌入式平台的项目结构
   - 去掉 GUI 依赖，改为事件驱动/状态机模式

2. **依赖安装脚本**
   - 针对 Ubuntu 20.04 aarch64 的安装脚本
   - 处理特殊依赖（dlib、PyTorch ARM64）

3. **核心代码移植**
   - 音频录制/播放适配 ALSA
   - 语音交互主循环
   - 各模块整合

4. **硬件驱动对接**（如需要）
   - 麦克风降噪板
   - USB 声卡
   - 舵机 PWM 控制

5. **测试与调试**
   - 分模块测试
   - 端到端集成测试

---

## 现有代码参考

可直接复用的核心代码片段，我会提供：

1. ASR WebSocket 协议实现
2. TTS WebSocket 双向流实现
3. Chat 流式调用
4. 声纹识别完整实现
5. 人脸识别管理器
6. YOLO 物体检测器
7. 意图处理器
8. Mem0 客户端

请在开始前确认：
- [x] 主控已通过 SSH 连接
- [x] Python 版本 (推荐 3.8+)
- [x] 网络连通性测试
- [ ] 音频设备识别情况
- [ ] 摄像头设备识别情况

---

## 开始部署

准备好后，请输入：

```
开始部署陪伴机器人
```

我会根据你的硬件环境逐步指导完成部署。
